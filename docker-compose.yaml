version: '3.8'

services:
  # --- Source Database ---
  postgres-source:
    image: debezium/postgres:15-alpine # Pre-configured for logical replication
    container_name: postgres-source
    hostname: postgres-source
    ports: ["5432:5432"]
    environment: { POSTGRES_USER: user, POSTGRES_PASSWORD: password, POSTGRES_DB: source }
    volumes:
      - pgdata-source:/var/lib/postgresql/data
      - ./postgres-init-scripts:/docker-entrypoint-initdb.d
    networks: [data-pipeline-net]
    healthcheck: { test: ["CMD-SHELL", "pg_isready -U user -d source"], interval: 10s, timeout: 5s, retries: 5 }

  # --- Kafka Infrastructure ---
  zookeeper:
    image: confluentinc/cp-zookeeper:7.3.2
    container_name: zookeeper
    hostname: zookeeper
    ports: ["2181:2181"]
    environment: { ZOOKEEPER_CLIENT_PORT: 2181, ZOOKEEPER_TICK_TIME: 2000 }
    networks: [data-pipeline-net]

  # --- CDC Tool: Debezium via Kafka Connect ---
  kafka:
    image: confluentinc/cp-kafka:7.3.2
    container_name: kafka
    hostname: kafka
    ports: ["29092:29092"] # Port for host access
    depends_on: [zookeeper, postgres-source]
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_CONFLUENT_LICENSE_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_CONFLUENT_BALANCER_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
    networks: [data-pipeline-net]

  connect:
    image: debezium/connect:2.5 # Use the Debezium Connect image
    container_name: connect
    hostname: connect
    ports: ["8083:8083"] # Kafka Connect REST API
    depends_on:
      postgres-source:
        condition: service_healthy
    environment:
      BOOTSTRAP_SERVERS: kafka:9092 # Point to Kafka broker
      GROUP_ID: connect-cluster-group # Kafka Connect consumer group ID
      CONFIG_STORAGE_TOPIC: connect_configs
      OFFSET_STORAGE_TOPIC: connect_offsets
      STATUS_STORAGE_TOPIC: connect_statuses
      CONFIG_STORAGE_REPLICATION_FACTOR: 1
      OFFSET_STORAGE_REPLICATION_FACTOR: 1
      STATUS_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_KEY_CONVERTER_SCHEMAS_ENABLE: "false"
      CONNECT_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: "false"
      CONNECT_PLUGIN_PATH: /kafka/connect
    networks: [data-pipeline-net]

  # --- Destination Database: ClickHouse (Datalake/Data Warehouse) ---
  clickhouse:
    image: clickhouse/clickhouse-server:latest
    depends_on: [postgres-source]
    container_name: clickhouse
    hostname: clickhouse
    ports: ["8123:8123", "9000:9000"]
    environment:
      CLICKHOUSE_DB: L1_datalake
      CLICKHOUSE_USER: user
      CLICKHOUSE_PASSWORD: password
    volumes:
      - chdata:/var/lib/clickhouse
      - ./clickhouse_config/users.xml:/etc/clickhouse-server/users.d/my_users.xml
      - ./clickhouse_config:/docker-entrypoint-initdb.d
    ulimits: { nofile: { soft: 262144, hard: 262144 } }
    networks: [data-pipeline-net]
    healthcheck: {
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:8123/ping"],
      interval: 10s,
      timeout: 5s,
      retries: 5
    }

# --- Shared Network ---
networks:
  data-pipeline-net:
    driver: bridge

# --- Persistent Volumes ---
volumes:
  pgdata-source:
  chdata:
  airflow-pgdata: