version: '3.8'

# Common Airflow settings
x-airflow-common: &airflow-common
  image: apache/airflow:2.8.4 # Use a specific stable version
  environment:
    &airflow-common-env
    AIRFLOW__CORE__EXECUTOR: LocalExecutor
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow_user:airflow_pass@airflow-postgres:5432/airflow_db
    AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    AIRFLOW__WEBSERVER__SECRET_KEY: 'super-secret-key-please-change-me!' # CHANGE THIS FOR PRODUCTION!
    AIRFLOW_UID: '50000' # Run Airflow processes as non-root user
  volumes:
    - ./airflow-dags:/opt/airflow/dags
    - ./airflow-logs:/opt/airflow/logs
    - ./airflow-plugins:/opt/airflow/plugins
  networks:
    - data-pipeline-net
  depends_on:
    airflow-postgres:
      condition: service_healthy # Wait for Airflow DB to be ready
  user: "${AIRFLOW_UID:-50000}" # Set user in container

services:
  # --- Source Database ---
  postgres-source:
    image: debezium/postgres:15-alpine # Pre-configured for logical replication
    container_name: postgres-source
    hostname: postgres-source
    ports: ["5432:5432"]
    environment: { POSTGRES_USER: user, POSTGRES_PASSWORD: password, POSTGRES_DB: source }
    volumes:
      - pgdata-source:/var/lib/postgresql/data
      - ./postgres-init-scripts:/docker-entrypoint-initdb.d
    networks: [data-pipeline-net]
    healthcheck: { test: ["CMD-SHELL", "pg_isready -U user -d source"], interval: 10s, timeout: 5s, retries: 5 }

  # --- Kafka Infrastructure ---
  zookeeper:
    image: confluentinc/cp-zookeeper:7.3.2
    container_name: zookeeper
    hostname: zookeeper
    ports: ["2181:2181"]
    environment: { ZOOKEEPER_CLIENT_PORT: 2181, ZOOKEEPER_TICK_TIME: 2000 }
    networks: [data-pipeline-net]

  kafka:
    image: confluentinc/cp-kafka:7.3.2
    container_name: kafka
    hostname: kafka
    ports: ["29092:29092"] # Port for host access
    depends_on: [zookeeper, postgres-source]
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_CONFLUENT_LICENSE_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_CONFLUENT_BALANCER_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
    networks: [data-pipeline-net]

  # --- CDC Tool: Debezium via Kafka Connect ---
  connect:
    image: debezium/connect:2.5 # Use the Debezium Connect image
    container_name: connect
    hostname: connect
    ports: ["8083:8083"] # Kafka Connect REST API
    depends_on:
      postgres-source:
        condition: service_healthy
    environment:
      BOOTSTRAP_SERVERS: kafka:9092 # Point to Kafka broker
      GROUP_ID: connect-cluster-group # Kafka Connect consumer group ID
      CONFIG_STORAGE_TOPIC: connect_configs
      OFFSET_STORAGE_TOPIC: connect_offsets
      STATUS_STORAGE_TOPIC: connect_statuses
      CONFIG_STORAGE_REPLICATION_FACTOR: 1
      OFFSET_STORAGE_REPLICATION_FACTOR: 1
      STATUS_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_KEY_CONVERTER_SCHEMAS_ENABLE: "false"
      CONNECT_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: "false"
      CONNECT_PLUGIN_PATH: /kafka/connect
    networks: [data-pipeline-net]

  # --- Destination Database: ClickHouse (Datalake/DataWarehouse) ---
  clickhouse:
    image: clickhouse/clickhouse-server:latest
    depends_on: [postgres-source]
    container_name: clickhouse
    hostname: clickhouse
    ports: ["8123:8123", "9000:9000"]
    environment:
      CLICKHOUSE_DB: L1_datalake
      CLICKHOUSE_USER: user
      CLICKHOUSE_PASSWORD: password
    volumes:
      - chdata:/var/lib/clickhouse
      - ./clickhouse_config/users.xml:/etc/clickhouse-server/users.d/my_users.xml
      - ./clickhouse_config:/docker-entrypoint-initdb.d
    ulimits: { nofile: { soft: 262144, hard: 262144 } }
    networks: [data-pipeline-net]
    healthcheck: {
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:8123/ping"],
      interval: 10s,
      timeout: 5s,
      retries: 5
    }
#
#  # --- Orchestration: Apache Airflow ---
#  airflow-postgres: # Metadata database for Airflow
#    image: postgres:13
#    container_name: airflow-postgres
#    hostname: airflow-postgres
#    environment: { POSTGRES_USER: airflow_user, POSTGRES_PASSWORD: airflow_pass, POSTGRES_DB: airflow_db, PGDATA: /var/lib/postgresql/data/pgdata }
#    volumes: ["airflow-pgdata:/var/lib/postgresql/data/pgdata"]
#    networks: [data-pipeline-net]
#    healthcheck: { test: ["CMD-SHELL", "pg_isready -U airflow_user -d airflow_db"], interval: 10s, timeout: 5s, retries: 5 }
#
#  airflow-webserver:
#    <<: *airflow-common
#    container_name: airflow-webserver
#    command: bash -c "airflow db init && airflow webserver"
#    ports: ["8080:8080"] # Airflow UI
#    healthcheck: { test: ["CMD", "curl", "--fail", "http://localhost:8080/health"], interval: 30s, timeout: 10s, retries: 5 }
#    networks: [data-pipeline-net]
#
#  airflow-scheduler:
#    <<: *airflow-common
#    container_name: airflow-scheduler
#    command: bash -c "airflow db init && airflow scheduler"
#    depends_on:
#      airflow-postgres:
#        condition: service_healthy

# --- Shared Network ---
networks:
  data-pipeline-net:
    driver: bridge

# --- Persistent Volumes ---
volumes:
  pgdata-source:
  chdata:
  airflow-pgdata: